package org.example;

import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;
import java.util.Properties;

import org.apache.spark.ml.Pipeline;
import org.apache.spark.ml.PipelineModel;
import org.apache.spark.ml.PipelineStage;
import org.apache.spark.ml.evaluation.RegressionEvaluator;
import org.apache.spark.ml.feature.HashingTF;
import org.apache.spark.ml.feature.IDF;
import org.apache.spark.ml.feature.StopWordsRemover;
import org.apache.spark.ml.feature.Tokenizer;
import org.apache.spark.ml.feature.VectorAssembler;
import org.apache.spark.ml.recommendation.ALS;
import org.apache.spark.ml.recommendation.ALSModel;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;

public class nanomachineson {

    private static final String DB_URL = "jdbc:postgresql://localhost:15432/postgres";
    private static final String DB_USER = "zwloader";
    private static final String DB_PASSWORD = "0010085070Pgsql";

    // –î–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –ø–µ—Ä–µ–¥–∞—á–∏ –ø—É—Ç–∏ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç
    public static String modelPathArg = null;

    public static void main(String[] args) {
        if (args.length > 0) {
            modelPathArg = args[0];
            System.out.println("[ALS] –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å –¥–ª—è –º–æ–¥–µ–ª–∏: " + modelPathArg);
        }
        System.out.println("–ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Ñ–∏–ª—å–º–æ–≤...");

        // –°–æ–∑–¥–∞–Ω–∏–µ Spark —Å–µ—Å—Å–∏–∏
        SparkSession spark = SparkSession.builder()
                .appName("–°–∏—Å—Ç–µ–º–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Ñ–∏–ª—å–º–æ–≤")
                .master("local[*]")
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                .getOrCreate();

        // –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
        Properties connectionProperties = new Properties();
        connectionProperties.put("user", DB_USER);
        connectionProperties.put("password", DB_PASSWORD);
        connectionProperties.put("driver", "org.postgresql.Driver");

        try {
            // 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ñ–∏–ª—å–º–∞—Ö
            System.out.println("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ñ–∏–ª—å–º–∞—Ö...");
            Dataset<Row> moviesDF = loadMoviesData(spark, connectionProperties);

            // 2. –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ –æ—Ü–µ–Ω–∫–∏
            System.out.println("–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...");
            createTestData();

            // 3. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –æ—Ü–µ–Ω–∫–∏
            System.out.println("–ó–∞–≥—Ä—É–∂–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...");
            Dataset<Row> ratingsDF = loadRatingsData(spark, connectionProperties);

            if (ratingsDF.count() > 0) {
                // 4. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                System.out.println("–û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...");
                trainRecommendationModel(spark, ratingsDF);
            }

            // 5. –°–æ–∑–¥–∞–µ–º content-based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
            System.out.println("–°–æ–∑–¥–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞...");
            createContentBasedRecommendations(spark, moviesDF);

        } catch (Exception e) {
            System.err.println("–û—à–∏–±–∫–∞: " + e.getMessage());
            e.printStackTrace();
        } finally {
            spark.stop();
            System.out.println("–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!");
        }
    }

    // –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ —Ñ–∏–ª—å–º–∞—Ö
    private static Dataset<Row> loadMoviesData(SparkSession spark, Properties props) {
        String movieQuery = """
            SELECT 
                m.id,
                m.title,
                m.overview,
                m.vote_average,
                m.vote_count,
                m.popularity,
                m.release_year,
                STRING_AGG(g.name, ' ') as genres_text
            FROM movies m
            LEFT JOIN movie_genres mg ON m.id = mg.movie_id
            LEFT JOIN genres g ON mg.genre_id = g.id
            WHERE m.overview IS NOT NULL 
              AND m.vote_count > 10
            GROUP BY m.id, m.title, m.overview, m.vote_average, 
                     m.vote_count, m.popularity, m.release_year
            """;
//        LIMIT 10000
        Dataset<Row> moviesDF = spark.read()
                .jdbc(DB_URL, "(" + movieQuery + ") as movies_data", props);

        System.out.println("–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∏–ª—å–º–æ–≤: " + moviesDF.count());
        moviesDF.show(5, false);
        return moviesDF.cache(); // –ö—ç—à–∏—Ä—É–µ–º –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    }

    // –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    private static void createTestData() {
        try {
            Connection conn = DriverManager.getConnection(DB_URL, DB_USER, DB_PASSWORD);
            Statement stmt = conn.createStatement();

            // –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            String createUsers = """
                INSERT INTO users (username, age, gender) 
                SELECT 
                    'testuser_' || generate_series(1, 500),
                    (random() * 50 + 18)::int,
                    CASE WHEN random() > 0.5 THEN 'M' ELSE 'F' END
                ON CONFLICT DO NOTHING
                """;
            stmt.executeUpdate(createUsers);

            // –°–æ–∑–¥–∞–µ–º –æ—Ü–µ–Ω–∫–∏ —Ñ–∏–ª—å–º–æ–≤ —Å –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º
            String createRatings = """
                INSERT INTO user_ratings (user_id, movie_id, rating, created_at)
                SELECT 
                    u.id,
                    m.id,
                    CASE 
                        WHEN m.vote_average >= 8.5 THEN 
                            CASE WHEN random() < 0.7 THEN (random() * 1 + 4)::numeric(2,1) 
                                 ELSE (random() * 2 + 3)::numeric(2,1) END
                        WHEN m.vote_average >= 7.5 THEN 
                            CASE WHEN random() < 0.6 THEN (random() * 1.5 + 3.5)::numeric(2,1) 
                                 ELSE (random() * 2.5 + 2.5)::numeric(2,1) END
                        WHEN m.vote_average >= 6.5 THEN 
                            CASE WHEN random() < 0.5 THEN (random() * 1.5 + 3)::numeric(2,1) 
                                 ELSE (random() * 3 + 2)::numeric(2,1) END
                        WHEN m.vote_average >= 5.5 THEN (random() * 2.5 + 2)::numeric(2,1)
                        ELSE (random() * 3 + 1)::numeric(2,1)
                    END,
                    NOW()
                FROM users u
                CROSS JOIN (
                    SELECT id, vote_average 
                    FROM movies 
                    WHERE vote_average IS NOT NULL AND vote_count > 50
                    ORDER BY random() 
                    LIMIT 500
                ) m
                WHERE random() < 0.15  -- –ë–æ–ª—å—à–µ —Å–ø–∞—Ä—Å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
                ON CONFLICT DO NOTHING
                """;
            stmt.executeUpdate(createRatings);

            conn.close();
            System.out.println("–¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã");

        } catch (Exception e) {
            System.err.println("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: " + e.getMessage());
        }
    }

    // –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ü–µ–Ω–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    private static Dataset<Row> loadRatingsData(SparkSession spark, Properties props) {
        Dataset<Row> ratingsDF = spark.read()
                .jdbc(DB_URL, "user_ratings", props)
                .select("user_id", "movie_id", "rating");

        System.out.println("–ó–∞–≥—Ä—É–∂–µ–Ω–æ –æ—Ü–µ–Ω–æ–∫: " + ratingsDF.count());
        
        // –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ñ–∏–ª—å–º—ã —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—Ü–µ–Ω–æ–∫
        Dataset<Row> filteredRatings = ratingsDF
                .groupBy("user_id").count().filter("count >= 5")
                .join(ratingsDF, "user_id")
                .drop("count")
                .groupBy("movie_id").count().filter("count >= 3")
                .join(ratingsDF, "movie_id")
                .drop("count")
                .select("user_id", "movie_id", "rating");
        
        System.out.println("–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –æ—Ü–µ–Ω–æ–∫: " + filteredRatings.count());
        if (filteredRatings.count() > 0) {
            filteredRatings.show(10);
        }
        return filteredRatings.cache();
    }

    // –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    private static void trainRecommendationModel(SparkSession spark, Dataset<Row> ratingsDF) {
        // –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ –∏ —Ç–µ—Å—Ç
        Dataset<Row>[] splits = ratingsDF.randomSplit(new double[]{0.8, 0.2}, 42);
        Dataset<Row> training = splits[0].cache();
        Dataset<Row> test = splits[1].cache();

        System.out.println("–û–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: " + training.count());
        System.out.println("–¢–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤: " + test.count());

        // –ü—Ä–æ—Å—Ç–æ–π grid search –¥–ª—è –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        double bestRMSE = Double.MAX_VALUE;
        ALSModel bestModel = null;
        
        // –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        int[] ranks = {20, 50, 100};
        double[] regParams = {0.01, 0.1, 0.2};
        
        System.out.println("–ò—â–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã...");
        
        for (int rank : ranks) {
            for (double regParam : regParams) {
                System.out.printf("–¢–µ—Å—Ç–∏—Ä—É–µ–º: rank=%d, regParam=%.2f...%n", rank, regParam);
                
                ALS als = new ALS()
                        .setMaxIter(20)
                        .setRegParam(regParam)
                        .setRank(rank)
                        .setUserCol("user_id")
                        .setItemCol("movie_id")
                        .setRatingCol("rating")
                        .setColdStartStrategy("drop");

                ALSModel model = als.fit(training);
                Dataset<Row> predictions = model.transform(test);

                RegressionEvaluator evaluator = new RegressionEvaluator()
                        .setMetricName("rmse")
                        .setLabelCol("rating")
                        .setPredictionCol("prediction");

                double rmse = evaluator.evaluate(
                        predictions.filter("prediction IS NOT NULL AND NOT isnan(prediction)")
                );
                
                System.out.printf("RMSE: %.3f%n", rmse);
                
                if (rmse < bestRMSE) {
                    bestRMSE = rmse;
                    bestModel = model;
                    System.out.printf("‚úÖ –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: %.3f (rank=%d, regParam=%.2f)%n", 
                                    rmse, rank, regParam);
                }
            }
        }
        
        System.out.printf("üéØ –õ—É—á—à–∞—è –æ—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏ (RMSE): %.3f%n", bestRMSE);
        System.out.println("–ß–µ–º –º–µ–Ω—å—à–µ —ç—Ç–æ —á–∏—Å–ª–æ, —Ç–µ–º –ª—É—á—à–µ –º–æ–¥–µ–ª—å!");

        // –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        Dataset<Row> userRecs = bestModel.recommendForAllUsers(10);
        System.out.println("–¢–æ–ø —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:");
        userRecs.show(5, false);

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        try {
            // –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            String modelPath = System.getProperty("modelPath");
            if (modelPath == null || modelPath.isEmpty()) {
                modelPath = (org.example.nanomachineson.modelPathArg != null) ? org.example.nanomachineson.modelPathArg : "./models/movie_recommendation_model";
            }
            bestModel.save(modelPath);
            System.out.println("–õ—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: " + modelPath);
        } catch (Exception e) {
            System.out.println("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–æ–¥–µ–ª—å: " + e.getMessage());
        }
    }

    // Content-based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏–π –∏ –∂–∞–Ω—Ä–æ–≤
    private static void createContentBasedRecommendations(SparkSession spark, Dataset<Row> moviesDF) {
        System.out.println("–°–æ–∑–¥–∞–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Ñ–∏–ª—å–º–æ–≤...");

        // 1. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏–π (TF-IDF)
        Tokenizer tokenizer = new Tokenizer()
                .setInputCol("overview")
                .setOutputCol("words");

        StopWordsRemover remover = new StopWordsRemover()
                .setInputCol("words")
                .setOutputCol("filtered_words");

        HashingTF hashingTF = new HashingTF()
                .setInputCol("filtered_words")
                .setOutputCol("raw_features")
                .setNumFeatures(1000);

        IDF idf = new IDF()
                .setInputCol("raw_features")
                .setOutputCol("overview_features");

        // 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∂–∞–Ω—Ä–æ–≤
        Tokenizer genreTokenizer = new Tokenizer()
                .setInputCol("genres_text")
                .setOutputCol("genre_words");

        HashingTF genreHashingTF = new HashingTF()
                .setInputCol("genre_words")
                .setOutputCol("genre_features")
                .setNumFeatures(50);

        // 3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        VectorAssembler assembler = new VectorAssembler()
                .setInputCols(new String[]{"overview_features", "genre_features"})
                .setOutputCol("features");

        // –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        Pipeline pipeline = new Pipeline()
                .setStages(new PipelineStage[]{
                        tokenizer, remover, hashingTF, idf,
                        genreTokenizer, genreHashingTF, assembler
                });

        // –û–±—É—á–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        PipelineModel pipelineModel = pipeline.fit(moviesDF);
        Dataset<Row> featuresDF = pipelineModel.transform(moviesDF);

        System.out.println("–ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è " + featuresDF.count() + " —Ñ–∏–ª—å–º–æ–≤");
        featuresDF.select("id", "title", "features").show(5, false);

        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        try {
            pipelineModel.save("models/content_based_pipeline");
            System.out.println("–ü–∞–π–ø–ª–∞–π–Ω content-based —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω!");
        } catch (Exception e) {
            System.out.println("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–∞–π–ø–ª–∞–π–Ω: " + e.getMessage());
        }
    }
}
